{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Instruction for Graph Judgement\n",
    "Source: `target.source`, Output: `train_instructions_llama.json`\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Is this true: Philippine one hundred-peso note face value 100?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Yes, this is true.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Is this true: Philippine one hundred-peso note face value Philippine president?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"No, this is not true.\"\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Target: formulate json data for kg completion model\n",
    "Format: \n",
    "    [\n",
    "        {\n",
    "            \"instruction\": \"Is this true: ...?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"Yes, this is true.\" or \"No, this is not true.\"\n",
    "        },\n",
    "        ..\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import functools\n",
    "import ast\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# read the text to be denoised\n",
    "triples = []\n",
    "dataset_path = './GPT3.5_result_rebel_sub/' # ./GPT3.5_result_rebel/ ./GPT3.5_result_webnlg/\n",
    "\n",
    "# read triples\n",
    "with open(dataset_path + f'train.source', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        triples.append(ast.literal_eval(l.strip()))\n",
    "\n",
    "# generate training data \n",
    "res_list = []\n",
    "for triple_list in tqdm(triples):            \n",
    "    tail_list = [x[-1] for x in triple_list]\n",
    "    for idx in range(len(triple_list)):\n",
    "        if len(triple_list[idx]) == 1:\n",
    "            continue\n",
    "        elif len(triple_list[idx]) == 2:\n",
    "            inst_pos = f\"Is this true: {triple_list[idx][0]} {triple_list[idx][1]}\"\n",
    "            output_pos = \"Yes, this is true.\"\n",
    "            temp_dict_pos = {\"instruction\": inst_pos, \"input\": \"\", \"output\": output_pos}\n",
    "            res_list.append(temp_dict_pos)\n",
    "        else:\n",
    "            # positive instance\n",
    "            inst_pos = f\"Is this true: {triple_list[idx][0]} {triple_list[idx][1]} {triple_list[idx][2]}?\"\n",
    "            output_pos = \"Yes, this is true.\"\n",
    "            temp_dict_pos = {\"instruction\": inst_pos, \"input\": \"\", \"output\": output_pos}\n",
    "            res_list.append(temp_dict_pos)\n",
    "            \n",
    "            # negative instance----randomly select tail entity\n",
    "            neg_tail_list = [x for x in tail_list if x != triple_list[idx][2]]\n",
    "            if len(neg_tail_list) >= 1:\n",
    "                neg_tail = random.choice(neg_tail_list)\n",
    "                inst_neg = f\"Is this true: {triple_list[idx][0]} {triple_list[idx][1]} {neg_tail}?\"\n",
    "                output_neg = \"No, this is not true.\"\n",
    "                temp_dict_neg = {\"instruction\": inst_neg, \"input\": \"\", \"output\": output_neg}\n",
    "                res_list.append(temp_dict_neg)\n",
    "            \n",
    "# write into file\n",
    "with open(dataset_path + f'train_instructions_llama.json', 'w') as f:\n",
    "    json.dump(res_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate test data for Graph Judgement\n",
    "\n",
    "Generate test data with i-th iteration generated graphs\n",
    "\n",
    "Example:\n",
    "```csv\n",
    "prompt,response\n",
    "Is this true: Coburg Peak instance of Rocky peak?,**\n",
    "Is this true: Coburg Peak located in Erul Heights?,**\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Target: fomulate generated graph to the text data format in KG completion model.\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import functools\n",
    "import ast\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# read triples generated\n",
    "triples = []\n",
    "dataset_path = './GPT3.5_result_GenWiki-Hard/'\n",
    "Iteration = 3\n",
    "\n",
    "# read triples\n",
    "with open(dataset_path + f'Graph_Iteration{Iteration}/test_generated_graphs.txt', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        triples += ast.literal_eval(l.strip())\n",
    "\n",
    "# 写入到 CSV 文件\n",
    "with open(dataset_path + f'Graph_Iteration{Iteration}/test_instructions_llama2_7b_itr{Iteration}.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['prompt', 'response']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # 写入表头\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # 遍历数据\n",
    "    for triple in tqdm(triples):\n",
    "        if len(triple) != 3:\n",
    "            prompt = f\"Is this true: {' '.join(triple)}?\"\n",
    "        else:\n",
    "            subject, predicate, obj = triple\n",
    "            prompt = f\"Is this true: {subject} {predicate} {obj}?\"\n",
    "        response = \"**\"  # 可以对 response 进行更多的逻辑处理\n",
    "        writer.writerow({'prompt': prompt, 'response': response})\n",
    "\n",
    "print(\"CSV 文件已创建！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Target: fomulate generated graph to the text data format in KG completion model.\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import functools\n",
    "import ast\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# read triples generated\n",
    "triples = []\n",
    "dataset_path = './GPT3.5_result_GenWiki-Hard/'\n",
    "\n",
    "# read triples\n",
    "with open(dataset_path + f'gpt_baseline/test_generated_graphs.txt', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        try:\n",
    "            triples += ast.literal_eval(l.strip())\n",
    "        except:\n",
    "             triples += [['none', 'none', 'none']]\n",
    "        \n",
    "\n",
    "# 写入到 CSV 文件\n",
    "with open(dataset_path + f'gpt_baseline/test_instructions_llama2_7b_gpt.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['prompt', 'response']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # 写入表头\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # 遍历数据\n",
    "    for triple in tqdm(triples):\n",
    "        triple = [str(x) for x in triple]\n",
    "        if len(triple) != 3:\n",
    "            prompt = f\"Is this true: {' '.join(triple)}?\"\n",
    "        else:\n",
    "            subject, predicate, obj = triple\n",
    "            prompt = f\"Is this true: {subject} {predicate} {obj}?\"\n",
    "        response = \"**\"  # 可以对 response 进行更多的逻辑处理\n",
    "        writer.writerow({'prompt': prompt, 'response': response})\n",
    "\n",
    "print(\"CSV 文件已创建！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the generated graphs with Judgement result\n",
    "This is where we get the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Target: Remove the not correct triples generated from text.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import functools\n",
    "import ast\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "triples = []\n",
    "dataset_path = './GPT3.5_result_GenWiki-Hard/'\n",
    "Iteration = 3\n",
    "\n",
    "# read triples\n",
    "with open(dataset_path + f'Graph_Iteration{Iteration}/test_generated_graphs.txt', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        triples.append(ast.literal_eval(l.strip()))\n",
    "\n",
    "pred_res = pd.read_csv(dataset_path + f'Graph_Iteration{Iteration}/pred_instructions_llama2_7b_itr{Iteration}.csv', header=0, sep=',')\n",
    "res_list = []\n",
    "for index, data in tqdm(pred_res.iterrows()):\n",
    "    try:\n",
    "        response = data['generated'].lower()\n",
    "        if 'no' in response[:10] or 'false' in response[:10]:\n",
    "            res_list.append(False)\n",
    "        else:\n",
    "            res_list.append(True)\n",
    "    except:\n",
    "        res_list.append(False)\n",
    "\n",
    "new_triples = []\n",
    "i = 0\n",
    "for triple_list in triples:\n",
    "    new_triple_list = []\n",
    "    wrong_triple_list = []\n",
    "    for triple in triple_list:\n",
    "        if res_list[i]:\n",
    "            new_triple_list.append(triple)\n",
    "        else:\n",
    "            wrong_triple_list.append(triple)\n",
    "        i += 1\n",
    "    if len(new_triple_list) < 4:\n",
    "        new_triple_list = triple_list\n",
    "    new_triples.append(new_triple_list)\n",
    "\n",
    "\n",
    "with open(dataset_path + f'Graph_Iteration{Iteration}/test_generated_graphs_final.txt', 'w') as f:\n",
    "    for doc in new_triples:\n",
    "        f.write(str(doc).replace('\\n', '') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Target: Remove the not correct triples generated from text.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import functools\n",
    "import ast\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "triples = []\n",
    "dataset_path = './GPT3.5_result_GenWiki-Hard/'\n",
    "\n",
    "# read triples\n",
    "with open(dataset_path + f'gpt_baseline/test_generated_graphs.txt', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        try:\n",
    "            triples.append(ast.literal_eval(l.strip()))\n",
    "        except:\n",
    "             triples.append([['none', 'none', 'none']])\n",
    "\n",
    "\n",
    "pred_res = pd.read_csv(dataset_path + f'gpt_baseline/pred_instructions_llama2_7b_gpt.csv', header=0, sep=',')\n",
    "res_list = []\n",
    "for index, data in tqdm(pred_res.iterrows()):\n",
    "    try:\n",
    "        response = data['generated'].lower()\n",
    "        if 'no' in response[:10] or 'false' in response[:10]:\n",
    "            res_list.append(False)\n",
    "        else:\n",
    "            res_list.append(True)\n",
    "    except:\n",
    "        res_list.append(False)\n",
    "new_triples = []\n",
    "i = 0\n",
    "for triple_list in triples:\n",
    "    new_triple_list = []\n",
    "    for triple in triple_list:\n",
    "        if res_list[i]:\n",
    "            new_triple_list.append(triple)\n",
    "        i += 1\n",
    "    if len(new_triple_list) < 5:\n",
    "        new_triple_list = triple_list\n",
    "    new_triples.append(new_triple_list)\n",
    "\n",
    "\n",
    "with open(dataset_path + f'gpt_baseline/test_generated_graphs_final.txt', 'w') as f:\n",
    "    for doc in new_triples:\n",
    "        f.write(str(doc).replace('\\n', '') + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hhy_graphgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
